{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captionning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T12:42:21.821898Z",
     "start_time": "2023-02-03T12:42:21.813880Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, sys, torchvision\n",
    "from PIL import Image\n",
    "from dataloader.get import DataGetter\n",
    "sys.path.append('./LAVIS/')\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T12:43:22.862981Z",
     "start_time": "2023-02-03T12:43:21.129966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dg=DataGetter()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup device to use\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# we associate a model with its preprocessors to make it easier for inference.\n",
    "model, vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip_caption\", model_type=\"large_coco\", is_eval=True, device=device\n",
    ")\n",
    "# uncomment to use base model\n",
    "# model, vis_processors, _ = load_model_and_preprocess(\n",
    "#     name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device\n",
    "# )\n",
    "vis_processors.keys()\n",
    "\n",
    "def get_caption(image,device):\n",
    "    \n",
    "    raw_image = Image.fromarray(image)\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # we'll first use beam search to generate a caption\n",
    "    caption = model.generate({\"image\": image})\n",
    "    \n",
    "    # we'll then use nucleus sampling to get multiple captions\n",
    "    # and more precise captions (you can uncomment the line below)\n",
    "    \n",
    "    # caption = model.generate({\"image\": image}, use_nucleus_sampling=True, num_captions=3)\n",
    "    \n",
    "    # store the generated caption in a variable\n",
    "    return caption\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "c:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"c:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"c:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle 'weakref' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle 'weakref' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m cloudpickle\u001b[39m.\u001b[39;49mdumps(obj, pickle_protocol)\n\u001b[0;32m    459\u001b[0m \u001b[39mexcept\u001b[39;00m pickle\u001b[39m.\u001b[39mPickleError:\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     70\u001b[0m cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[0;32m     71\u001b[0m     file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m cp\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[0;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 602\u001b[0m     \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, obj)\n\u001b[0;32m    603\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle 'weakref' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m df_spark \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(df_test)\n\u001b[0;32m     40\u001b[0m \u001b[39m# Use the map transformation to apply the function to each row\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m df_captioned \u001b[39m=\u001b[39m df_spark\u001b[39m.\u001b[39;49mrdd\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m row: (row[\u001b[39m0\u001b[39;49m], row[\u001b[39m1\u001b[39;49m], caption_from_image_file(row[\u001b[39m1\u001b[39;49m])))\u001b[39m.\u001b[39;49mtoDF([\u001b[39m'\u001b[39;49m\u001b[39mfilename\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mimages\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcaptions\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     43\u001b[0m \u001b[39m# # # Convert the Spark dataframe back to a Pandas dataframe\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m# # df_captioned = df_captioned.toPandas()\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m# # Stop the Spark session\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39m# spark.stop()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m@no_type_check\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtoDF\u001b[39m(\u001b[39mself\u001b[39m, schema\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sampleRatio\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     76\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m sparkSession\u001b[39m.\u001b[39;49mcreateDataFrame(\u001b[39mself\u001b[39;49m, schema, sampleRatio)\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[1;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[0;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    896\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m    933\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, RDD):\n\u001b[1;32m--> 934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromRDD(data\u001b[39m.\u001b[39;49mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromLocal(\u001b[39mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[39mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 600\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchema(rdd, samplingRatio, names\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    601\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    602\u001b[0m     tupled_rdd \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmap(converter)\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\session.py:546\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_inferSchema\u001b[39m(\n\u001b[0;32m    526\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    527\u001b[0m     rdd: RDD[Any],\n\u001b[0;32m    528\u001b[0m     samplingRatio: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    529\u001b[0m     names: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    530\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructType:\n\u001b[0;32m    531\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[39m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     first \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39;49mfirst()\n\u001b[0;32m    547\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m first:\n\u001b[0;32m    548\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe first row in RDD is empty, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcan not infer schema\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m   1891\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1903\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   1904\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[0;32m   1905\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1882\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1883\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[0;32m   1885\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[0;32m   1886\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   1485\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD\u001b[39m.\u001b[39mrunJob(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[0;32m   1487\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3502\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3503\u001b[0m     profiler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 3505\u001b[0m wrapped_func \u001b[39m=\u001b[39m _wrap_function(\n\u001b[0;32m   3506\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd_deserializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd_deserializer, profiler\n\u001b[0;32m   3507\u001b[0m )\n\u001b[0;32m   3509\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3510\u001b[0m python_rdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonRDD(\n\u001b[0;32m   3511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prev_jrdd\u001b[39m.\u001b[39mrdd(), wrapped_func, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreservesPartitioning, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_barrier\n\u001b[0;32m   3512\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[39massert\u001b[39;00m serializer, \u001b[39m\"\u001b[39m\u001b[39mserializer should not be empty\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3361\u001b[0m command \u001b[39m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[1;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[39m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m   3363\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3364\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonFunction(\n\u001b[0;32m   3365\u001b[0m     \u001b[39mbytearray\u001b[39m(pickled_command),\n\u001b[0;32m   3366\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3371\u001b[0m     sc\u001b[39m.\u001b[39m_javaAccumulator,\n\u001b[0;32m   3372\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   3342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prepare_for_python_RDD\u001b[39m(sc: \u001b[39m\"\u001b[39m\u001b[39mSparkContext\u001b[39m\u001b[39m\"\u001b[39m, command: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mbytes\u001b[39m, Any, Any, Any]:\n\u001b[0;32m   3343\u001b[0m     \u001b[39m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[0;32m   3344\u001b[0m     ser \u001b[39m=\u001b[39m CloudPickleSerializer()\n\u001b[1;32m-> 3345\u001b[0m     pickled_command \u001b[39m=\u001b[39m ser\u001b[39m.\u001b[39;49mdumps(command)\n\u001b[0;32m   3346\u001b[0m     \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3347\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pickled_command) \u001b[39m>\u001b[39m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mgetBroadcastThreshold(sc\u001b[39m.\u001b[39m_jsc):  \u001b[39m# Default 1M\u001b[39;00m\n\u001b[0;32m   3348\u001b[0m         \u001b[39m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    466\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCould not serialize object: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (e\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, emsg)\n\u001b[0;32m    467\u001b[0m print_exec(sys\u001b[39m.\u001b[39mstderr)\n\u001b[1;32m--> 468\u001b[0m \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mPicklingError(msg)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle 'weakref' object"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def caption_from_image_file(x):\n",
    "    return [get_caption(i,device) for i in x.load()]\n",
    "\n",
    "df = dg.getData(\"train\")\n",
    "\n",
    "df_test = df.head(5)\n",
    "\n",
    "\n",
    "# df_test['captions'] = df_test.images.apply(caption_from_image_file)\n",
    "\n",
    "# df_test.to_csv('test.csv',index=False)\n",
    "\n",
    "# # free up cuda memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# convert all columns that are objects to string\n",
    "df_test = df_test.astype({col: 'str' for col in df_test.columns[df_test.dtypes == 'object']})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-19\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:/Program Files/Spark/spark-3.3.1-bin-hadoop3\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ParallelCaptioning\").getOrCreate()\n",
    "\n",
    "# Convert the Pandas dataframe to a Spark dataframe\n",
    "df_spark = spark.createDataFrame(df_test)\n",
    "\n",
    "# Use the map transformation to apply the function to each row\n",
    "df_captioned = df_spark.rdd.map(lambda row: (row[0], row[1], caption_from_image_file(row[1]))).toDF(['filename', 'images', 'captions'])\n",
    "\n",
    "# # # Convert the Spark dataframe back to a Pandas dataframe\n",
    "# # df_captioned = df_captioned.toPandas()\n",
    "\n",
    "# # # Write the data to a CSV file\n",
    "# # df_captioned.to_csv('test_parallel.csv', index=False)\n",
    "\n",
    "# # Stop the Spark session\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31ff630c6e82ba12fef539dea8022209045e49d81c27e1972fa5386bff428379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
