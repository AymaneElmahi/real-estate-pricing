{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captionning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T12:42:21.821898Z",
     "start_time": "2023-02-03T12:42:21.813880Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch, sys, torchvision\n",
    "from PIL import Image\n",
    "from dataloader.get import DataGetter\n",
    "sys.path.append('./LAVIS/')\n",
    "from lavis.models import load_model_and_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T12:43:22.862981Z",
     "start_time": "2023-02-03T12:43:21.129966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "dg=DataGetter()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# setup device to use\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "\n",
    "# we associate a model with its preprocessors to make it easier for inference.\n",
    "model, vis_processors, _ = load_model_and_preprocess(\n",
    "    name=\"blip_caption\", model_type=\"large_coco\", is_eval=True, device=device\n",
    ")\n",
    "# uncomment to use base model\n",
    "# model, vis_processors, _ = load_model_and_preprocess(\n",
    "#     name=\"blip_caption\", model_type=\"base_coco\", is_eval=True, device=device\n",
    "# )\n",
    "vis_processors.keys()\n",
    "\n",
    "def get_caption(image,device):\n",
    "    \n",
    "    raw_image = Image.fromarray(image)\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # we'll first use beam search to generate a caption\n",
    "    caption = model.generate({\"image\": image})\n",
    "    \n",
    "    # we'll then use nucleus sampling to get multiple captions\n",
    "    # and more precise captions (you can uncomment the line below)\n",
    "    \n",
    "    # caption = model.generate({\"image\": image}, use_nucleus_sampling=True, num_captions=3)\n",
    "    \n",
    "    # store the generated caption in a variable\n",
    "    return caption\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# def caption_from_image_file(x):\n",
    "#     return [str(get_caption(i,device)) for i in x.load()]\n",
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# df = dg.getData(\"train\")\n",
    "\n",
    "# df_test = df.head(5)\n",
    "\n",
    "# # # start timer  \n",
    "# # import time\n",
    "# # start_time = time.time()\n",
    "\n",
    "\n",
    "# # df_test['captions'] = df_test.images.apply(caption_from_image_file)\n",
    "\n",
    "# # # end timer (in minutes)\n",
    "# # print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "# # df_test.to_csv('test.csv',index=False)\n",
    "\n",
    "# # # free up cuda memory\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # df_test.captions\n",
    "\n",
    "\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# # Create a Dask dataframe from a Pandas dataframe\n",
    "# df_dask = dd.from_pandas(df_test, npartitions=4)\n",
    "\n",
    "# # Apply the `caption_from_image_file` function to the `images` column using the `map` method\n",
    "# df_dask['captions'] = df_dask['images'].map(caption_from_image_file, meta=('captions', 'f8'))\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Convert the Dask dataframe back to a Pandas dataframe\n",
    "# df_result = df_dask.compute()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # import cv2\n",
    "# # import numpy as np\n",
    "\n",
    "# # df = dg.getData(\"train\")\n",
    "# # df_test = df.head(5)\n",
    "\n",
    "# # # start timer\n",
    "# # import time\n",
    "# # start_time = time.time()\n",
    "\n",
    "# # def caption_from_image_file(img):\n",
    "# #     return str(get_caption(img, device))\n",
    "\n",
    "# # import multiprocessing as mp\n",
    "# # from multiprocessing import Pool\n",
    "\n",
    "# # pool = Pool(mp.cpu_count())\n",
    "# # df_test['captions'] = pool.map(caption_from_image_file, df_test.images)\n",
    "\n",
    "# # print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "# # df_test.to_csv('test.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/opt/spark-3.2.1-bin-hadoop3.2/python\")\n",
    "# sys.path.append(\"/opt/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.4-src.zip\")\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# findspark.find()\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk-19\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"C:/Program Files/Spark/spark-3.3.1-bin-hadoop3\"\n",
    "\n",
    "# def get_captions_spark(df):\n",
    "#     # Create a Spark session\n",
    "#     spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "#     print(\"Spark session created\")\n",
    "    \n",
    "#     # Create a DataFrame from the input df\n",
    "#     images_df = spark.createDataFrame(df)\n",
    "    \n",
    "#     # Split the DataFrame into batches of size 50\n",
    "#     images_rdd = images_df.rdd.mapPartitions(lambda x: [x[1:] for x in x]).flatMap(lambda x: x).zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "#     captions_rdd = images_rdd.map(lambda x: (x[0], [get_caption(im, device) for im in x[1].load()]))\n",
    "\n",
    "#  # Convert the RDD back to a DataFrame\n",
    "#     captions_df = captions_rdd.toDF([\"index\", \"captions\"])\n",
    "    \n",
    "#     # Join the DataFrames\n",
    "#     result_df = images_df.join(captions_df, \"index\").drop(\"index\")\n",
    "    \n",
    "#     return result_df.toPandas()\n",
    " \n",
    "# df = dg.getData(\"train\")\n",
    "# df_test = df.head(5)\n",
    "\n",
    "# print(df_test.dtypes)\n",
    "\n",
    "# # delete all object columns except the images column\n",
    "# object_cols = df_test.select_dtypes(include=['object']).columns.tolist()\n",
    "# object_cols.remove('images')\n",
    "# df_test = df_test.drop(object_cols, axis=1)\n",
    "\n",
    "# # start timer\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# df_test = get_captions_spark(df_test)\n",
    "# print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m df\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mgetData(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m ds\u001b[39m=\u001b[39mdf[:\u001b[39m100\u001b[39m]\n\u001b[1;32m---> 34\u001b[0m all_captions \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39;49mrun(get_column_for_captions_async(ds, batch_size\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m))\n\u001b[0;32m     35\u001b[0m ds[\u001b[39m'\u001b[39m\u001b[39mcaptions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mall_captions\n\u001b[0;32m     36\u001b[0m \u001b[39mprint\u001b[39m(ds)\n",
      "File \u001b[1;32mc:\\Users\\elmah\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m coroutines\u001b[39m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39ma coroutine was expected, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def get_caption_async(image):\n",
    "    captions = get_caption(image, device)\n",
    "    return captions\n",
    "\n",
    "async def get_captions_batch_async(row, device):\n",
    "    tasks = []\n",
    "    for im in row.load():\n",
    "        tasks.append(get_caption_async(im))\n",
    "    captions = await asyncio.gather(*tasks)\n",
    "    return captions\n",
    "\n",
    "\n",
    "\n",
    "async def get_column_for_captions_async(ds, batch_size=50):\n",
    "    captions = []\n",
    "    all_captions=[]\n",
    "    dj=ds['images']\n",
    "    \n",
    "    for i in range(len(dj)):\n",
    "        batch_captions = await get_captions_batch_async(dj.iloc[i], device)\n",
    "        all_captions.append(batch_captions)\n",
    "    \n",
    "    return all_captions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df=DataGetter()\n",
    "    df=df.getData('train')\n",
    "    ds=df[:100]\n",
    "    all_captions = asyncio.run(get_column_for_captions_async(ds, batch_size=50))\n",
    "    ds['captions']=all_captions\n",
    "    print(ds)\n",
    "    ds.to_csv('tab.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31ff630c6e82ba12fef539dea8022209045e49d81c27e1972fa5386bff428379"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
